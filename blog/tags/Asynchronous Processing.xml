<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title><![CDATA[CX Social Dev Blog]]></title>
    <link href="/blog/tags/Asynchronous Processing.xml" rel="self"/>
    <link href="/"/>
    <updated>2017-11-02T08:54:36+00:00</updated>
    <id>/</id>
        <generator uri="http://sculpin.io/">Sculpin</generator>
            <entry>
            <title type="html"><![CDATA[When too many API calls are breaking your app]]></title>
            <link href="/blog/2017/11/02/javascript-deferred"/>
            <updated>2017-11-02T10:00:00+00:00</updated>
            <id>/blog/2017/11/02/javascript-deferred</id>
            <content type="html"><![CDATA[<p>A few weeks ago, I was confronted with a strange issue regarding the "followers" component we have in our inbox.
It's a cool feature that gives you the possibility to check which connected Twitter account is following or followed by another Twitter account. Most accounts or users don't have that many connected Twitter profiles but of course there are always some exceptions. One of our customers had more than 100 connected profiles. The process to check the relationship between the profiles is rather easy. We do a call to the back-end for every profile. In the back-end we do an API call to Twitter to fetch the relationship details. So you can imagine it takes some time if we have to do this for many profiles. In the front-end the component became very slow and it looked like the component crashed.</p>

<p><img src="/images/2017-11-02-javascript-deferred/followers.png" alt="CX Social followers dropdown in inbox" /></p>

<p>After finding out what the problem is, the next step is of course solving it. I started with checking the back-end for some improvements. Instead of calling Twitter's API for every single profile, I checked if we could ask for multiple relations at once. Sadly this wasn't the case. So this trail was a dead end.</p>

<p>If we can't change the API, we could try to avoid using the API as much as possible. By caching the result for some time, we could avoid requesting the same data multiple times as this also improves speed. This wasn't a good option either because the received data was already cached by us in the back-end.</p>

<p>The only thing I could still think of was trying to reduce the calls to the Twitter API or to our own API. Changing the back-end would be a lot of work because the endpoint is used in a lot of places in the front-end. So the only option I wanted to consider was making a change in the one place where we had the problem. In the inbox we loop over the profiles and do a seperate call for every profile. For every call to our API, we wait until we have the response before doing a new call. So instead of doing synchronous calls, we should consider doing the calls asynchronously.</p>

<p>Not knowing if this was possible in Javascript, I started searching and fell upon the <code>Promise</code> object, which does exactly what we need. However, <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise">the documentation for Promise on MDN</a> quickly revealed that it's not supported by some browsers we still have to support. After some more digging around the internet, we stumbled upon the <code>Deferred</code> object in jQuery which does almost the same, and works in most browsers. From its documentation:</p>

<blockquote>
  <p>It can register multiple callbacks into callback queues, invoke callback queues, and relay the success or failure state of any synchronous or asynchronous function</p>
</blockquote>

<p>So what we actually have to do is start all the calls to the back-end at once. Once we did that we create a new <code>Deferred</code> object. When all the <code>combinedCalls</code> are done, we can start updating our component. It only has to do one big update and it doesn't take that much time because the calls are all fired at the same time instead of one by one.</p>

<pre><code class="js">var profileIds = this.profileIds;

var calls = _.map(services, function (service) {
    var url = '/twitter/relation?accounts='+ service.id +'&amp;users=' + profileIds;

    return $.get(url);
});

var combinedResult = $.Deferred();

var combinedCalls = $.when.apply($, calls);

combinedCalls.done(function() {

    var resultsOfDefferedAjaxCalls = _.toArray(arguments).slice(0, calls.length);

    var relations = _.map(resultsOfDefferedAjaxCalls, function(ajaxResult) {
        return ajaxResult.result.data;
    });

    combinedResult.resolve(relations);
});

combinedCalls.fail(function() {
    // Combine args so that we can pass reason for
    // failure to reject()
    combinedResult.reject();
});

return combinedResult.promise();
</code></pre>

<p>So with some small changes we improved a lot on the loading time of the dropdown. The next problem was that some calls could take some time and if there is one slow call, all the other calls have to wait till the slowest call is finished. So I had to find another solution for this new problem. So instead of just calling the endpoint I added a callback function to the call.</p>

<pre><code class="js">var calls = _.map(services, function (service) {
    var url = '/twitter/relation?accounts='+ service.id +'&amp;users=' + profileIds;

    var onGoingCall = $.get(url);

    onGoingCall.done(callback);

    return onGoingCall;
});
</code></pre>

<p>The moment the call is finished, the callback is triggered and the component will update. You don't have to worry about slow calls because the component will update as soon as possible and the customer can see that data is loading and updating.</p>

<p>That's how we fixed this for now, hopefully we can move to the more modern approach of using <code>Promise</code> in the near future!</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Distributed Processing via RabbitMQ]]></title>
            <link href="/blog/2017/01/07/distributed-processing-via-rabbit-mq"/>
            <updated>2017-01-07T08:00:00+00:00</updated>
            <id>/blog/2017/01/07/distributed-processing-via-rabbit-mq</id>
            <content type="html"><![CDATA[<p>Looking at <a href="https://stackshare.io/clarabridge-cx-social/web-application-stack">the tech stack of our CX Social product</a>, there's tens of components involved, and this post will go into detail about one of the most central components of our system: our RabbitMQ service.</p>

<p>RabbitMQ, or at least the concept of "Message Brokers" it implements, is central to the way we capture and digest the hundreds of social media messages we track each second, but also in how we throw around workload and tasks. Actually, it's so central we often take it for granted. So, let's take a step back and see why we use it ...</p>

<h2 id="what-is-a-queueing-system%3F">What is a Queueing System?</h2>

<p>Almost too simple to explain, a queueing system is a service where you can pile up a bunch of things to do (tasks) or messages to process (social media tweets etc. in our case). Different processes put messages on the queue (publish) while multiple other processes (consumers), possibly on other servers, read from these queues to e.g. execute a certain process.</p>

<p><center><img src="/images/2017-01-07-distributed-processing-via-rabbit-mq/queue.png" alt="What Is A Queue?" /></center></p>

<p>At Clarabridge CX Social, we use <a href="https://www.rabbitmq.com/">RabbitMQ</a>, but there's also <a href="https://kafka.apache.org/">Apache Kafka</a> or <a href="https://aws.amazon.com/sqs/">Amazon SQS</a> as often used alternatives.</p>

<p>Below is a screenshot of our RabbitMQ dashboard, showing a.o. about 3000 messages per second being processed.</p>

<p><img src="/images/2017-01-07-distributed-processing-via-rabbit-mq/rabbitmq-dashboard.png" alt="RabbitMQ Dashboard" /></p>

<h2 id="so%2C-what%27s-cool-about-queues%3F">So, What's Cool About Queues?</h2>

<h3 id="1.-do-stuff-async-and-handle-peaks">1. Do Stuff Async and Handle Peaks</h3>

<p>If at any point in our code we can procrastinate, we try to do so. If it's not super important to execute a certain function immediately, we push a message onto one of our RabbitMQ queues containing enough information to execute the function later, potentially by someone else. Easy. And lazy.</p>

<p>An example where we do this, is in the processing of webhooks for Facebook. For every like or comment that happens on one of the Facebook Pages that our CX Social clients own, Facebook sends us a real-time update containing who commented/liked which post. This information needs to be added to our clients' inboxes as soon as possible, but it's not vital that that's done in the same webhook-request that Facebook sends us. So, this page just pushes whatever information it gets from Facebook onto a queue. And an army of consumers for this queue turns these messages in e.g. comment objects to add to our data cluster.<br />
Whenever a Facebook post we monitor turns out to be hugely popular, and hundreds of post interactions per second is no exception, this queue serves both as a buffer (so we can handle these peak moments) and as a distributor (so multiple processes can massage and index the data).</p>

<p>Other common use cases include emailing or notifying several people. Say you have an online shop, and you want to notify a whole bunch of subscribers as soon as you add a new item to the inventory. Instead of sending these notifications in the same web request that the admin adds a product in, you push the notification-job on a queue, and let a different process send the actual emails or push notifications. Your initial request ("Add product x") stays fast, while all necessary emails are sent in the background.</p>

<h3 id="2.-split-up-complex-problems">2. Split up Complex Problems</h3>

<p>Queues are also particularly handy to split up heavy tasks into smaller, more manageable chunks of work. By using queues as the glue between these steps, you create more visibility into this chain of events.</p>

<p>One such chain that is vital to CX Social is the processing of a new incoming message (a tweet, an Instagram photo, etc.). Whenever a new messages is capture by our data crawling systems, there's several steps we need to do to get it to our clients, including:</p>

<ol>
<li>Checking which of our clients are interested in the message by matching it to the set-up of their accounts.<br />
(E.g. does it contain any of the keywords they're monitoring? Is it from a social profile they own?)</li>
<li>Resolving any of the urls in the post, as to get more details about the linked sites.</li>
<li>Computing the language the post's text is in.</li>
<li>Making an estimated guess of what the sentiment of the post could be.</li>
<li>Predict any labels that the client uses often for this particular post.</li>
<li>Indexing it to our main data cluster (ElasticSearch).</li>
<li>Predicting a possible reply for the post, if actionable.</li>
<li>Sending a notification to logged in users to their interfaces update with the new data.</li>
</ol>

<p>Each step of this process involves different services, of which some might be external, or slow, or down. By splitting this big chunk of work into smaller steps that are executed in sequence (each step generates an event that the next step listens to), we have better control over the execution of these steps. In addition, each step is simpler to monitor and thus debug (both in resource usage as speed).</p>

<h3 id="3.-give-certain-jobs-priorities">3. Give Certain Jobs Priorities</h3>

<p>By using different queues for different types of tasks, and by playing around with the amount of consumer-processes you have running for a certain queue, you have control over the priority of the tasks you push to this service, and different queues and types of tasks don't affect each other.</p>

<h3 id="4.-cross-service-communication">4. Cross-Service Communication</h3>

<p>We also use RabbitMQ in case different services need to talk to each other. At Clarabridge CX Social we have a service for real-time communication to a user's browser using websockets (via node.js/Socket.io). Every other service in our cluster can just send their messages there via RabbitMQ.
If a certain php process needs to send a real-time notification to a certain logged in user, the php process publishes an event to a RabbitMQ queue. A node.js server will then consume the event, and passes it on the client via socket.io.<br />
Using RabbitMQ as the intermediate we allow services written in a multitude of different languages to talk to each other.</p>

<h3 id="5.-monitoring-made-easy">5. Monitoring Made Easy</h3>

<p>A benefit of architecting your application this way, is that your Message Broker becomes an important and easy service to monitor, giving early indications when problems arise. As soon as a certain queue is growing beyond its expected volume, it's often an indication of a process that is slower than expected, or fully down.</p>

<p>If you monitor ...</p>

<ul>
<li>the amount of messages on a queue at any time;</li>
<li>the age of the oldest messages on a queue;</li>
<li>the rate of publishing to a queue;</li>
<li>the rate of consuming from a queue;</li>
</ul>

<p>... you have a very good indication of the health and performance of your application.</p>

<p><img src="/images/2017-01-07-distributed-processing-via-rabbit-mq/serverdensity-dashboard.png" alt="ServerDensity Dashboard" /></p>
]]></content>
        </entry>
    </feed>