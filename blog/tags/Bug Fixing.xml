<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title><![CDATA[Clarabridge Engage Dev Blog]]></title>
    <link href="/blog/tags/Bug Fixing.xml" rel="self"/>
    <link href="/"/>
    <updated>2020-10-23T07:56:42+00:00</updated>
    <id>/</id>
        <generator uri="http://sculpin.io/">Sculpin</generator>
            <entry>
            <title type="html"><![CDATA[A tale of a small JavaScript bug]]></title>
            <link href="/blog/2019/10/02/a-tale-of-a-small-javascript-bug"/>
            <updated>2019-10-02T14:30:00+00:00</updated>
            <id>/blog/2019/10/02/a-tale-of-a-small-javascript-bug</id>
            <content type="html"><![CDATA[<p>Sometimes it's assumed that the frontend is a relatively harmless place to be programming. We've been proven wrong once again.</p>

<h2 id="%F0%9F%94%A5%F0%9F%90%B6%E2%98%95%EF%B8%8F%F0%9F%94%A5-this-is-fine">üî•üê∂‚òïÔ∏èüî• This is fine</h2>

<p><img src="/images/2019-10-02-a-tale-of-a-small-javascript-bug/this-is-fine.png" alt="This is fine" /></p>

<p>For a few months now, we have been seeing relatively high loads on our web servers. The loads varied from around 90% of the total CPU power available, to a bit over 100% in peak hours. We assumed that this was normal and that our product was just being used more (which it was), and that this was a natural burden on the load on our servers. The approach we were going to take was to continue scaling horizontally.</p>

<p>Two weeks ago however, we noticed huge daily load spikes (up to 1000% of our capacity) during peak hours. We got alert SMS'es of Redis that couldn't handle the amount of requests, and saw Kibana logs of very slow user requests to certain routes, effectively rendering our application unusable at times. It was "all hands on deck" immediately, and we started digging. Before we could find out what was happening it stopped again, and the load dropped to what we assumed was "normal load". We found, also using Kibana, that there was a huge amount of calls to our <code>/find</code> endpoint, and so we decided to implement a <a href="/blog/2017/05/02/sliding-window-rate-limiter-redis/">rate limiter</a> on those, to prevent our app from going down, and buy us some time to look for the actual issue. However, we couldn't see why that endpoint was hit so hard.</p>

<p><img src="/images/2019-10-02-a-tale-of-a-small-javascript-bug/kibana.png" alt="Kibana showing a lot of requests for one user in a short time frame" /></p>

<h2 id="%F0%9F%90%9B%F0%9F%95%B5%EF%B8%8F%E2%80%8D%E2%99%82%EF%B8%8F--bug-hunting">üêõüïµÔ∏è‚Äç‚ôÇÔ∏è  Bug Hunting</h2>

<p>A few days later, when we were hit with a huge amount of calls again (this time, they were ratelimited and not producing those huge loads on our servers, but we could see them in the logs and in Kibana), we noticed that most of them were coming from only a limited number of customers. We tried to work in our app as they would do, but couldn't reproduce it. After some time however, we noticed in the network tab of the browser we were working in, that the same call to the <code>/find</code> endpoint was done multiple times simultaneously! Bingo!</p>

<p>Or not Bingo? The code seemed to be fine: When we mount our inbox's react component, we use <code>Sonora.on(eventname, callback)</code> to bind a callback to a given websocket eventname (Sonora is an abstraction around <a href="https://socket.io/">socket.io</a>). When we unmount the react component again, we call <code>Sonora.off(eventname, callback)</code> to stop listening for those events. When such an event comes in, we would potentially need more info from the backend and a call to the <code>/find</code> endpoint is issued. It definitely looked like we didn't unregister the callbacks when unmounting that component, given how we saw multiple calls being made simultaneously in the console's network tab, whenever a websocket message came in.</p>

<p><img src="/images/2019-10-02-a-tale-of-a-small-javascript-bug/performance-tab.png" alt="Browser's performance tab showing lots of calls to the find endpoint" /></p>

<p>While everything was looking okay in the consuming code of the Sonora <code>.on()</code> and <code>.off()</code> methods, we concluded that something must've been wrong in the wrapper around socket.io itself. When looking inside the <code>.on()</code> method, we found out that there was a debugging statement added like this:</p>

<pre><code class="javascript">const Sonora = {
    // ...

    on: (event, callback) =&gt; {
        socket.on(
            event,
            () =&gt; {
                console.log('some debugging here');

                callback.apply(this, arguments);
            }
        );
    }

    // ...
}
</code></pre>

<p>As you can see, we wrapped the actual callback inside an anonymous function, and passed that on to Socket.io's <code>.on()</code> handler. Now, when calling <code>.off()</code>, we sent along the original callback which didn't match the wrapped one, and nothing was removed. Since it's possible to have multiple callbacks for each incoming event, this resulted in the same callback being added time after time and not being removed. So we had basically made all our clients do <em>loads</em> of unnecessary calls to <code>/find</code> by adding a debugging statement! And believe it or not, this debugging statement was there for a while! (Thanks git blame!)</p>

<h2 id="%F0%9F%94%A8%F0%9F%91%A9%E2%80%8D%F0%9F%94%A7-fixing-it">üî®üë©‚Äçüîß Fixing it</h2>

<p>The fix was easy enough: don't add that anonymous function.</p>

<p>The load on our servers dropped immediately when we put that small change into production, and not only during peak hours. It seemed that we had been on a tipping point. A few more users online at any given time, a few more open tabs with our application running in them. The servers constantly running at semi-high loads. And then we went over it. üî•</p>

<p><img src="/images/2019-10-02-a-tale-of-a-small-javascript-bug/loads-dropping.png" alt="Graph displaying the huge drop in load on our servers" /></p>

<p>Along the way we did some other optimisations:</p>

<ul>
<li>we implemented a rate limiter on the <code>/find</code> endpoint</li>
<li>we disallowed to register exactly the same callback twice for the same event using <code>Sonora.on()</code></li>
<li>we fixed a second bug in <code>Sonora.off()</code> where we didn't remove the correct callbacks sometimes</li>
</ul>

<h2 id="%F0%9F%98%87%F0%9F%92%AD-and-they-lived-happily-ever-after">üòáüí≠ And they lived happily ever after</h2>

<p>This was the tale of the small JavaScript bug bringing down the huge web application, a modern David and Goliath if you will. Frontend debugging <em>tricks</em> can take down your application! Every change is important, certainly changes that happen in code that's used very frequently. Some changes that look inconspicuous can over time become real bottlenecks. Tools like Kibana and the browser profiler &amp; network tabs really helped us a great deal finding the issue, so don't forget what you have at your disposal.</p>

<p>We hope you enjoyed reading about our failures! Happy debugging! üëã</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Experiment: Pair Programming By Default]]></title>
            <link href="/blog/2018/12/07/one-month-of-pairing"/>
            <updated>2018-12-07T16:00:00+00:00</updated>
            <id>/blog/2018/12/07/one-month-of-pairing</id>
            <content type="html"><![CDATA[<p>As you could already read in some of our <a href="/blog/2017/05/22/implementing-little-big-details-on-offsite-bootcamp">previous</a> <a href="/blog/2017/03/01/batman-chatman">blogposts</a>, we are a relatively small team, with some loose rules for working together. Up until one month ago, everyone just picked a project from the roadmap and started implementing, and there was a rotation system within the team to always have two people available to help out our support team to assess bugs &amp; fix issues respectively.</p>

<h2 id="%F0%9F%92%AD%F0%9F%98%B0-about-silos-%26-issue-fatigue">üí≠üò∞ About Silos &amp; Issue-Fatigue</h2>

<p>As you can imagine, this works really well for projects that are small and well defined, and for teams where everyone is of the same skill level, and has the same development practices. People are not in each other's ways, and you can "horizontally scale" your team. The only trouble is, we're a small team, we have senior and junior developer profiles, and our projects are mostly not so small. The result of this, is that the person implementing a feature is most likely the only one that <strong>really</strong> knows the feature inside out. So the <a href="https://en.wikipedia.org/wiki/Bus_factor">bus factor</a> for every feature was basically 1. For some features we had insane knowledge silos.</p>

<p>There's a similar story to the support rotation system within the team: the people talking to support and fixing bugs most likely didn't implement the features where the bugs appeared. If they did, the bugs were quickly fixed, and if not, people had to try and understand code written by someone else that they'd never seen <strong>over and over</strong> again. Not ideal. And the result was that for me personally, weeks when I was Batman (yes that's how we call the lucky engineer that's solving bugs that week) were the most draining periods of the year.</p>

<h2 id="%E2%9A%97%EF%B8%8F%F0%9F%94%AC-the-experiment">‚öóÔ∏èüî¨ The Experiment</h2>

<p>We do weekly roundups, and after an exhausting week of being Batman, I, expressed my feelings towards the Silos and the Batman weeks. The whole team actually agreed that the situation was not perfect, and we decided it was time for an experiment:</p>

<ul>
<li>Split up the (already small) team in two groups of people (3 to 4 people per group) that will always try to work together on the same feature.</li>
<li>Start every day together with your team (at the same computer or over <a href="https://get.slack.help/hc/en-us/articles/216771908-Make-calls-in-Slack">Slack Voice Calls</a>) trying to solve issues or create features using Pair Programming techniques. Split up the team for "monkey-jobs".</li>
<li>Both teams provide one team member per week to be Batman or Robin. They form their own team and tackle bugs together.</li>
</ul>

<p>We limited the scope of the experiment to the winter release, so 3 months. So far we did a (very limited) retrospective every week.</p>

<h2 id="%F0%9F%91%A9%E2%80%8D%F0%9F%92%BB%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB-what-have-we-learned-so-far%3F">üë©‚Äçüíªüë®‚Äçüíª What have we learned so far?</h2>

<p>We've only been doing this for 4 weeks, so I might be jumping to conclusions here. So far the experience has been positive!</p>

<ul>
<li>It's great to be able to verify your ideas immediately, and to try and build a shared understanding of the problem you're working on.</li>
<li>Pair Programming is exhausting, but it works! We feel the velocity gain and the concentration boost. We find that we're writing better code. We feel productive.</li>
<li>We're aware that it's exhausting and we take a lot of breaks, stop early when we're tired.</li>
<li>We're using Slack as team communication tool, and when someone is working at home, we can just keep Pair Programming because of the built-in voice calls and screen sharing. Slack even allows you to share your mouse and keyboard with the person at the other side of the call (that is, if you've installed Slack from their website instead of from the App Store).</li>
<li>We found this great blogpost with some basics to <a href="https://madewithlove.be/how-to-keep-pair-programming-digestible/">keep Pair Programming digestible</a>, by my friend <a href="https://twitter.com/woutersioen">Wouter Sioen</a>.</li>
<li>We've done some ad-hoc pair programming in the past, but starting the day at the same computer really helps you keep it up and encourages you to do it more often.</li>
<li>Batman and Robin are <em>way faster</em> at finding and fixing bugs than when they were working alone. Having that extra developer by your side improves your bug finding experience massively. When working alone, overlooking a small mistake could take up hours, while your pair could spot that in seconds. Our CTO <a href="https://twitter.com/oemebamo">Jurriaan Persyn</a> pointed out that we've fixed a third more issues than last month, even some that were open for a long time already!</li>
</ul>

<h2 id="%F0%9F%93%9A%F0%9F%A7%90-some-resources">üìöüßê Some resources</h2>

<ul>
<li><a href="https://tuple.app/pair-programming-guide/">Tuple's Pair Programming Guide with lots of good tips!</a></li>
<li><a href="http://www.davefarley.net/?p=267">Blog: Pair Programming For Introverts</a></li>
<li><a href="https://madewithlove.be/how-to-keep-pair-programming-digestible/">Blog: How To Keep Pair Programming Digestible</a></li>
<li><a href="https://www.martinfowler.com/bliki/PairProgrammingMisconceptions.html">Blog: Martin Fowler on Pair Programming Misconceptions</a></li>
<li><a href="http://wiki.c2.com/?ExtremeProgramming">WikiWikiWeb on Extreme Programming</a></li>
<li><a href="https://twitter.com/MrAlanCooper/status/1060558122916278272">Great twitter thread by @MrAlanCooper about Pair Programming</a></li>
<li><a href="https://twitter.com/mathiasverraes/status/1063063502254936065">Mathias Verraes talking about Pair Programming for quality</a></li>
</ul>

<p>We hope we can keep it up! ü§û</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Bug Fixing: Creating Synergy Between Dev and Support]]></title>
            <link href="/blog/2017/03/01/batman-chatman"/>
            <updated>2017-03-01T09:00:00+00:00</updated>
            <id>/blog/2017/03/01/batman-chatman</id>
            <content type="html"><![CDATA[<p>This blog post is about bugs. About bug-tracking software. About prioritising what issues to fix first. About client-developer communication. About Service Level Agreements. And about hotfixes.</p>

<p><center><img src="/images/2017-03-01-batman-chatman/oh-f.gif" alt="Oh, f*ck" /></center></p>

<p>If that didn't scare you away, well hello there!</p>

<h2 id="first-line-support">First Line Support</h2>

<p>Clarabrige Engage is software that allows companies to respond to their incoming social data (Twitter mentions, Facebook wall posts, etc.) as fast and as efficiently as possible. The customer care agents of companies like Telenet, De Lijn, NMBS (in Belgium), and Lufthansa, Turkish Airlines, T-Mobile (worldwide) use Clarabrige Engage day-in day-out. They've set-up highly customized workflows within our tool, including automated actions, advanced filtering and real-time communication.<br />
This makes Clarabrige Engage a business-critical tool for our clients to serve their customers. To help with set-up, or if clients have discovered a bug in our system, there is direct in-app access to a live chat that puts them in contact with one of our support agents.<br />
These support agents handle several chats and incoming emails per hour. We use Olark for in-app live chat, and use Salesforce Desk as the tool to handle incoming emails.</p>

<p><center><img src="/images/2017-03-01-batman-chatman/olark-chat.png" alt="Clarabrige Engage in-app Live Chat via Olark" /></center></p>

<h2 id="an-empowered-support-agent-means-less-bugs">An Empowered Support Agent Means Less Bugs</h2>

<p>The support agents are the people in our company that know our tool best. In several cases even better than us developers who wrote the code for it. We involve our support agents in testing new features we build. By making support part of the Q&amp;A-process, we assure they have used new features before clients put their hands on it, but also that the product team keeps solving <em>real</em> problems that our clients are having every day.<br />
Apart from that, we actively maintain FAQ documents that help support agents identify problems, and collect the correct information to reproduce issues. We - try to - regularly organise trainings that go deep into certain aspects of our application. The customer support agents for Clarabrige Engage know the basics of html, can interpret some - if not most - of our monitoring dashboards and know about <code>oAuth</code> or <code>API's</code> and find their way around the Chrome Developer Tools.<br />
All with one goal: to be able to solve our clients' problems as fast as possible. (Read: With as little help from a developer as possible üòè )</p>

<h2 id="a-direct-line-to-development">A Direct Line To Development</h2>

<p>For those cases where the help of a developer is wanted, our support team has a direct line open to development, and that's the internal Slack channel <code>#supdev</code> that all support agents have joined. The main goal of this channel is to discuss whether a certain <em>symptom</em> is in fact an issue. We've seen it work best if all these discussions happen through a single and public channel so that multiple people can jump in to help. This encourages learning from each other's questions and answers, and also helps to hand-over problems to teams in other timezones.
On the other hand, on active days, the constant flow of messages in the <code>#supdev</code> channel often get in the way of focus. That's why the people in our development team are taking turns in answering support questions in this channel, and by doing that allowing others to preserve their focus. The whole development team (all full stack developers) takes part in this schedule we nicknamed the 'chatman', in periods of one week.</p>

<p>The result of this direct line is;</p>

<ol>
<li>Less bugs:

<ul>
<li>Because some configuration errors might be spotted earlier;</li>
<li>Because multiple reports by clients get bundled into a single bug report;</li>
</ul></li>
<li>Better bug reports:

<ul>
<li>More detailed steps to reproduce;</li>
</ul></li>
</ol>

<h2 id="github-as-bug-repository">GitHub as Bug Repository</h2>

<p>We use GitHub Issues as our main bug tracker. Not that it matters that much, but here's some of the reasons we appreciate GitHub:</p>

<ul>
<li>Integration with the git repository;

<ul>
<li>Closing issues from your commit message via <code>closes #issueno</code>;</li>
<li>The automatic referencing between issues, pull requests and commits;</li>
</ul></li>
<li>It's simple and easy to use interface; and at the same time flexible enough due to its labeling system.</li>
<li>Its simple and extensive REST API.</li>
</ul>

<p>We have set-up an <a href="https://help.github.com/articles/creating-an-issue-template-for-your-repository/">issue template</a> to guide our support agents to provide as much of the relevant context as possible. This issue template makes sure we always know the User ID and Account ID of the person having/reporting the problem.</p>

<p>We have several types of issues; "critical", "minor" and "major", which we indicate with <a href="https://help.github.com/articles/creating-and-editing-labels-for-issues-and-pull-requests/">issue labels</a>. Most issues are classified as "minor", and the "major"/"critical" labels are reserved for those "drop everything and fix this"-type of problems.</p>

<p>Whenever a Desk case (= client report) results in a GitHub issue (= actual bugreport), we make sure to save the connection between the Desk case and GitHub issue in our own database. <em>(The why is explained later.)</em></p>

<p>In a typical week at Clarabrige Engage around 10 bugs are reported, and while most of them take about 2 to 3 hours to fix, some take days.</p>

<h2 id="batman-will-fix-it">Batman Will Fix It</h2>

<p>Similar to how we have a dedicated person answering questions from our support team, the person(s) responsible for fixing bugs is also a rotated role. Each week different people are assigned the Batman role, and spend their week digging into the reported and confirmed issues, trying to find a solution for them.</p>

<p>This separation of bug fixing work from standard product development <em>(i.e. new features, or reworked features)</em> has the following advantages for us:</p>

<ul>
<li><p>Everyone in the team will at some point be involved in fixing bugs, including bugs in parts of the code he/she doesn't know yet. Of course, everyone's free to go ask other people's advice, but it's definitely an efficient way of preventing knowledge silos. This contributes to everyone's knowledge of the code base, and also to how much care is given to writing clean &amp; well documented code.</p></li>
<li><p>In weeks a developer isn't assigned the Batman role, he/she can fully focus on the product development project; making it easier to stay focused and work towards target dates. Before we used a system like this, it was often much harder to make progress on product development, because there's always a certain bug that got more priority.</p></li>
<li><p>By separating these duties it also becomes more measurable how much effort is spent - team-wide - on each type of job, and how we should distribute our resources.</p></li>
</ul>

<p><center><img src="/images/2017-03-01-batman-chatman/scars.jpg" alt="Do you want to know how I got these scars?" /></center></p>

<p>At the same time, a system like this creates a few problems as well:</p>

<ul>
<li><p>Probably the person trying to find the cause of a bug is not the person who originally wrote the code, and thus it might take more time than somebody who knows the ins and outs of what he/she wrote.</p></li>
<li><p>Bugs that aren't fixed by the end of the week, will of course not go away by itself, and a handover to the next batman is needed.</p></li>
</ul>

<p>The solution for this is simple: allow to deviate from the guidelines. In emergency situations, more people are working on bugs than just the Batman. And if you are close to a fix on Friday evening and fixing it will only take another half a day; of course that's what we do. And have regular in-person <code>#supdev</code>-meetings with the support team, and this and next week's Batman and Chatman.<br />
The agenda for these meetings is:</p>

<ul>
<li>What are the most important issues still needing a fix, from a support point of view?</li>
<li>What are blocking problems, from a dev point of view?</li>
<li>Passing on information to the next batman, both from support and from batman.</li>
</ul>

<p>We use this system in a tech team of 11 people, of which 7 people are involved in this Batman/Chatman schedule. The rest of the team is either designer, data scientist or ops people. While it works for us now, we realise it might become hard if the team grows.</p>

<p>On top of the GitHub API we built several dashboards that plot the trend in amount of open bugs and that show our average resolution time. These are the prime indicators we take into account when planning extra resources for this role. Our Slack bot will also notify us when issues stay open for too long, or are unassigned, or miss labels, etc.</p>

<p><center><img src="/images/2017-03-01-batman-chatman/issue-stats.png" alt="Amount of open issues over time" /></center></p>

<p>When an issue is closed but without a resolution (it's a duplicate, we were unable to reproduce it, it's a limitation of the external services we use, etc.) we indicate this with a resolution label. And similarly we tag all bugs with a label indicating which Clarabrige Engage feature group it's related to. These two types of labels help us assess the quality of both our bug reports as identify problem areas with the application.</p>

<h2 id="prioritising-issues">Prioritising Issues</h2>

<p>The nature of a bug typically entails its priority; if something is a problem with a core feature of Clarabrige Engage, for multiple accounts at once, it's labeled as "major", and thus has higher priority over bugs classified as "minor". Luckily, the amount of "major" bugs is limited. So, most of the time we spent fixing "minor" bugs. Typically we have between 10 and 30 of these bugs open, and from this to-do list for the Batman it's important we focus on those issues that have the most impact, for whatever reason. It's our support team that decides which of these to work on, since they know a bug's impact on customers best.</p>

<p><center><img src="/images/2017-03-01-batman-chatman/github-screenshot.png" alt="Some open issues" /></center></p>

<h2 id="what-didn%27t-work">What Didn't Work</h2>

<p>We didn't always work this way. Over the course of the last 5 years, we've had a whole bunch of different approaches.</p>

<p>In our first iterations Batman also did the duties of who we now refer to as Chatman. But complex bugs often require a lot of focus (and we found out: more so than for standard product development work), so this combined role, resulted in a lower bug closing efficiency. And that's of course the main target for a system like this.</p>

<p>Not having a Chatman at all, is even worse. The fact that a developer can add his/her perspective to a bug report, greatly enriches the quality of a bug report. The fact that this communication with the Chatman happens over chat, avoids a slow ping-pong of "needs more information" comments on GitHub issues.</p>

<p>While it's sometimes tempting to skip a weekly <code>#supdev</code>-meetings, we've found that not having a regular in-person meeting between development and support creates an invisible "hostility" where it feels like one party is fighting against the other: fixing bugs versus reporting bugs. While what really happens is two teams working together to fix customer issues. Talking in person can cause breakthroughs because ideas get validated or ping-ponged.</p>

<h2 id="deploying-fixes-%26-reaching-out-to-the-customer">Deploying Fixes &amp; Reaching Out to the Customer</h2>

<p>When a developer has fixed a bug, it's pushed to a separate branch which is then opened up for peer review via <a href="https://help.github.com/articles/about-pull-requests/">GitHub's Pull Requests</a> and the Support Agents involved in the bug report are notified. At this moment it's also possible to fire up a staging environment with the bug fix to verify by support.</p>

<p>Approved bug fixes are typically pushed out to production the same day. And by requiring issues to be closed via GitHub's <code>closes #issueno</code> commits, our continuous integration agent Jenkins also knows what fixes he is deploying to production. We have configured Jenkins so that it leaves a comment on the actual GitHub issue with the build number and exact time of when a fix for that issue went live. Jenkins will also make sure to post any bug fixes that are deployed to a few of the relevant Slack channels.</p>

<p><center><img src="/images/2017-03-01-batman-chatman/fixed-comment.png" alt="It's fixed" /></center></p>

<p>This Jenkins-comment on the GitHub issue will also trigger the reopening of any Desk cases associated with the GitHub issue, so that our support team immediately has the relevant cases reopened. By closing the loop back to the customer we make sure that customer immediately knows about a fix, and that we have confirmation of the fix as soon as possible.</p>

<p>What strategies have worked for you to speed up the bug fixing process and ensure positive communication between those encountering the bugs, reporting the bugs and fixing the bugs?</p>

<p><center><img src="/images/2017-03-01-batman-chatman/fixed-slack.png" alt="It's fixed" /></center></p>
]]></content>
        </entry>
    </feed>